---
layout: post
title: Maximum Manifold Capacity Representations
categories:
- Research
- SENAI Lab
tags:
- geometry
- manifold
- representations
hidden: false
math: true
description: The paper proposes a new self-supervised learning (SSL) method which
  uses the representation of the data in higher manifolds and the manifold capacity
  to generate a loss function. This method is competing with the current state-of-art
  methods.
date: 2025-09-12 23:24 +0530
---
**Paper:** Learning Efficienct Coding of Natural Images with Maximum Manifold Capacity Representations \\
**Authors:** [Thomas Yerxa](https://scholar.google.com/citations?user=n4Uu99gAAAAJ&hl=en), [Yilun Kuang](https://scholar.google.com/citations?hl=en&user=XvIasgEAAAAJ), [Eero Simoncelli](https://scholar.google.com/citations?hl=en&user=MplR7_cAAAAJ), [SueYeon Chung](https://scholar.google.com/citations?hl=en&user=h7yVv0QAAAAJ) \\
**Link:** [pdfversion](https://arxiv.org/abs/2303.03307)

---

## Summary

The paper proposes a new method in the paradigm of self-supervised learning (SSL). In this paper the researchers propose a loss function $\mathcal{L} = -\|C\|_*$ where $C$ is the centroid matrix of the learned representations of the augmented data obtained from the image data and $\|\|_*$ is the nuclear norm (the sum of singular values of the matrix). The motivation of this loss function comes from the manifold representation and the the *Manifold Capacity* $\alpha_c$. Once the number of manifolds per dimension crosses this critical value the probability of identifying a linear boundary abruptly goes to zero.

* The following image captures the main idea of the paper
![capacity](/assets/img/mmcr/low_cap_vs_high_cap.png)

---

## Key Ideas

- Using `high-rank` approach in contrast to the `low-rank + logistic regression` which is ingeneral used in contrastive learning methods.
- Identifying a computable loss function solely based on the centroid matrix.
- Experimentally proving that in the process of pushing the centroids of different classes away, these classes are coming closer internally.

---

## Methodology

Instead of other loss functions that are generally used the authors used the new loss function and tested the representations that are learned for various downstream tasks and they also compared these with existing current state-of-art models.

### Manifold Capacity

If there are $P$ manifolds in a $D$ dimensional space then the value $\frac{P}{D}$ (which in number of manifolds per dimension) can be used as a representative for whether there exists a hyperplane that seperates two randomly chosen dichotomy. That is there is a limiting value $\alpha_c$ such that if $\frac{P}{D} < \alpha_c$ this hyperplane exists with a probability approximately $1$ and, if $\frac{P}{D} > \alpha_c$ then the probability of existence of such a hyperplane is close to $0$. This limiting value is said to be the manifold capacity.

The Manifold Capacity $\rightarrow \alpha_c$ depends on the following factors:
- $R_M$ the radius of the manifold (with respect to the centroid)
- $D_M$ the dimensionality of the manifold
- The cosine similarity of the centroids of various classes and if this is low then $\alpha_c$ can be approximated as $\phi(R_M\sqrt(D_M))$ where $\phi$ is a decreasing function.

In general it is hard to calculate these parameter for a random manifold. But if we assume that the manifolds are elliptical then these values can be analytically computed as

$$R_M = \sqrt{\sum_i\lambda_i^2}, D_M = \frac{(\sum_i\lambda_i)^2}{\sum_i\lambda_i^2}$$

where $\lambda_i^2$ is the eigenvalues of the covariance matrix of the manifold.

### Nuclear Norm

Observe the above expression and the approximation given for $\alpha_c$ combining these we get that the manifold capacity can be given by $\sum_i\lambda_i = \sum_i\sigma_i(C)$ the singular values of the matrix containing the points of the manifold. The $L_1$ norm of the singular matrix is said to be the nuclear norm of the matrix.

This connection is used and this is easily computable so this can be used as a loss function. The minimization of the nuclear norm of the centroid matrix is like trying to bring the manifold into a lower-dimension with the constraint of the manifold capacity - so this approach is `high-rank` which addresses the problem of dimensional-collapse in case of self-supervised learning methods.

---

## Results

The authors successfully gave a new loss function and also established the condition for optimality of that loss function which goes as following.

> Under the proposed loss function, the left singular values of an optimal embedding $Z^*$, are the eigenvectors of $G$, and the singular values of $Z^*$ are proportional to the top $d$ eigenvalues of $G$. Where $G$ captures the importance of across the augmented images generated from the given set of images.

* Different metrics of the augmented images
![Augmentation](/assets/img/mmcr/augmentation.png)

* Cosine Similarity
![cosine_similarity](/assets/img/mmcr/cosine_similarity.png)

* Evolution of different parameters with training
![Evolution](/assets/img/mmcr/evolution.png)

* Experimental evidence that the individual manifolds are coming closer as the training is progressing
![Experiment](/assets/img/mmcr/experiment.png)

* The Geometry of representations across layers - observe that the capacity is maximum and dimensional compression is also maximum in MMCR compared to others.
![Geometry](/assets/img/mmcr/geometry.png)



---

## Strengths

- Effectively captures the relation between the geometry of the representation and the singular values.
- Computationally effective loss function.
- Characterising the optimality of the loss function geometrically.

---

## Takeaways

From this paper I developed an understanding about the following topics
* Self-supervised Learning
    * Contrastive Methods
    * Non-Contrastive Methods
* Manifold Capacity
    * Maximum Manifold Capacity Representations

I would love to understand how the objective of moving the centroids away is inherently making the individual manifolds compact and more aligned.

---

## Questions / Discussion

- Why is it the case that the objective of moving the centroids farther is pushing the images of the same classes closer?
- What are the downstream tasks that are more benefitted from this objective?
- In what kind of learning does this objective function have more impact compared to other objective functions?
